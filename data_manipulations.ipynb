{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econ 191 Paper: Analyzing FOMC Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping, Cleaning and Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we being the process outlined in III.II (Data). We go through the process of scraping the data, converting the PDFs into text, using the Porter algorithm to stem words, removing stop words, counting and weighting relevant words by tf-idf (per document), and grouping each vector to the corresponding outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Outcome Variables\n",
    "Here we import our outcome variables from the the \"outcome_variables.csv\" file located in the same directory as this file. We skip the outcome data pre-processing part for ease of analysis.\n",
    "\n",
    "Our outcome data for the federal funds rate was taken directly from the Federal Reserve Bank of St. Louisâ€™ FRED portal. Our gross domestic product data was from the US Bureau of Economic Analysis. Unemployment and inflation rate calculated using the consumer price index were taken from the US Bureau of Labor Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv of data to pandas dataframe 1980 - 2011.\n",
    "outcome_vars = pd.read_csv(\"./outcome_data.csv\")\n",
    "# trim data to relevant years\n",
    "df_rel_years = outcome_vars[outcome_vars[\"Year\"] >= 1982]\n",
    "# gets rid of post-2008 years which might complicate data (as there is no exact target)\n",
    "df_outcome_vars = df_rel_years[df_rel_years[\"Federal Funds Target Rate\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate our outcome variable, Change in Federal Funds Rate, and append it to the table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# add change in Fed funds rate column\n",
    "changes = np.diff(np.array(df_outcome_vars[\"Federal Funds Target Rate\"]))\n",
    "df_outcome_vars[\"Change in Fed Funds Target Rate\"] = np.concatenate([[None], changes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping FOMC Transcripts and Generating Paths\n",
    "Here we scrape the transcripts, organizing them in the feddata/ directory by year. Within each year contains all the conference calls and meeting transcripts. In total, it amounts to 660 MB of data, or 18,741 pages. \n",
    "\n",
    "This scraper will not check if the files already exist, and will restart the scraping process if re-run. ** Do not run the scraper portion of this if you already have the data  **. It took so long I had to scrape this overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the appropriate boolean to True and the others to false to generate the appropriate dictionary for use later on. Excuse the poorly written code - it works but I haven't had the time to refactor.\n",
    "\n",
    "If you need to download the data, set \"link_to_file_on_website\" to True. Then run the scraping portion to download all the pdfs.\n",
    "\n",
    "If you have the text data, then set \"path_to_local_txt\" to True and skip the scraping process. *** If you're coming from the github repo, you have the .txt data ***. Just continue and ignore the sections that I say to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates a dictionary of appropriate transcript paths\n",
    "# if you already have the text data, set path_to_local_txt to True. \n",
    "link_to_file_on_website = False\n",
    "path_to_local_pdf = False\n",
    "path_to_local_txt = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this after selecting the appropriate boolean. We'll need this later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year Complete:  1982\n",
      "Year Complete:  1983\n",
      "Year Complete:  1984\n",
      "Year Complete:  1985\n",
      "Year Complete:  1986\n",
      "Year Complete:  1987\n",
      "Year Complete:  1988\n",
      "Year Complete:  1989\n",
      "Year Complete:  1990\n",
      "Year Complete:  1991\n",
      "Year Complete:  1992\n",
      "Year Complete:  1993\n",
      "Year Complete:  1994\n",
      "Year Complete:  1995\n",
      "Year Complete:  1996\n",
      "Year Complete:  1997\n",
      "Year Complete:  1998\n",
      "Year Complete:  1999\n",
      "Year Complete:  2000\n",
      "Year Complete:  2001\n",
      "Year Complete:  2002\n",
      "Year Complete:  2003\n",
      "Year Complete:  2004\n",
      "Year Complete:  2005\n",
      "Year Complete:  2006\n",
      "Year Complete:  2007\n",
      "Year Complete:  2008\n"
     ]
    }
   ],
   "source": [
    "if link_to_file_on_website:\n",
    "    base_url = \"https://www.federalreserve.gov/monetarypolicy/\"\n",
    "if path_to_local_pdf or path_to_local_txt:\n",
    "    base_directory = \"./feddata/\"\n",
    "    \n",
    "transcript_links = {}\n",
    "for year in range(1982, 2009): # from 1982 - 2008\n",
    "    \n",
    "    if link_to_file_on_website:\n",
    "        path = \"fomchistorical\" + str(year) + \".htm\"\n",
    "        html_doc = requests.get(base_url + path)\n",
    "        soup = BeautifulSoup(html_doc.content, 'html.parser')\n",
    "        links = soup.find_all(\"a\", string=re.compile('Transcript .*'))\n",
    "        link_base_url = \"https://www.federalreserve.gov\"\n",
    "        transcript_links[str(year)] = [link_base_url + link[\"href\"] for link in links]\n",
    "        \n",
    "    elif path_to_local_pdf or path_to_local_txt:\n",
    "        files = []\n",
    "        path_to_folder = base_directory + str(year)\n",
    "        new_files = os.walk(path_to_folder)\n",
    "        for file in new_files:\n",
    "            for f in file[2]:\n",
    "                if path_to_local_pdf:\n",
    "                    if f[-3:] == \"meeting.pdf\":\n",
    "                        files.append(str(file[0]) + \"/\" + f)\n",
    "                elif path_to_local_txt:\n",
    "                    if f[-11:] == \"meeting.txt\":\n",
    "                        files.append(str(file[0]) + \"/\" + f)\n",
    "        transcript_links[str(year)] = files\n",
    "    print(\"Year Complete: \", year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below is the *** scraping process *** to download the links. This will work if you selected the \"link_to_file_on_website\" boolean. *** Do NOT run if you have the .txt files ***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for year in transcript_links.keys():\n",
    "#     if not os.path.exists(\"./feddata/\" + year):\n",
    "#         os.makedirs(\"./feddata/\" + year)\n",
    "#     for link in transcript_links[year]:\n",
    "#         response = urllib.request.urlopen(str(link))\n",
    "#         name = re.search(\"[^/]*$\", str(link))\n",
    "#         print(link)\n",
    "#         with open(\"./feddata/\" + year + \"/\" + name.group(), 'wb') as f:\n",
    "#             f.write(response.read())\n",
    "#         print(\"file uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just downloaded the data, you need to convert it to text. The most accurate translation is from this website:\n",
    "\n",
    "pdftotext.com\n",
    "\n",
    "Download the text data and put it in the corresponding year folder. It should appear next to the pdf. Then re-start the process from \"Scraping FOMC Transcripts and Generating Paths\" section. Now that you have the text files, you should set \"path_to_local_txt\" to True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create sorted list of transcripts\n",
    "The ordering in this list will be important, as we will generate a corresponding list with weighted word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents 216\n"
     ]
    }
   ],
   "source": [
    "# create list of all paths and sort in increasing order\n",
    "sorted_transcripts = []\n",
    "for linkset in transcript_links.values():\n",
    "    sorted_transcripts += linkset\n",
    "sorted_transcripts = sorted(sorted_transcripts)\n",
    "print(\"Number of Documents\", len(sorted_transcripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stop words from txt files\n",
    "We do this seperately from our tfidf vectorizer, as we want to run a stemmer. This generates new text files with original name, but with \"Stop.txt\" appended. For example:\n",
    "\n",
    "FOMC19820202meeting.txt -> \n",
    "FOMC19820202meetingStop.txt\n",
    "\n",
    "*** if you already have the Stop.txt files then do not run this. It will take a long time***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# i = 0\n",
    "# for f in sorted_transcripts:\n",
    "#     infile = open(f, 'r')\n",
    "#     text = infile.readlines()\n",
    "#     newfile = open(f[:-4] + 'Stop.txt','w')\n",
    "#     new_text = []\n",
    "#     for line in text:\n",
    "#         mod_line = line[:-1].split(\" \")\n",
    "#         new_line = [word for word in mod_line if word.lower() not in stopwords.words('english')]\n",
    "#         new_string = \"\"\n",
    "#         for word in new_line:\n",
    "#             new_string += \" \" + word\n",
    "#         new_string += \"\\n\"\n",
    "#         new_text += new_string\n",
    "#     newfile.writelines(new_text)\n",
    "#     newfile.close()\n",
    "#     infile.close()\n",
    "#     i += 1\n",
    "#     print(\"File \" + str(i) + \" of \" + str(len(sorted_transcripts)) + \" Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now re-adjust our sorted_transcripts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents 216\n"
     ]
    }
   ],
   "source": [
    "# run all the adjusting of variable sorted_transcripts in order\n",
    "mod_transcripts = []\n",
    "for link in sorted_transcripts:\n",
    "    mod_transcripts.append(str(link)[:-4] + \"Stop.txt\")\n",
    "sorted_transcripts = mod_transcripts\n",
    "print(\"Number of Documents\", len(sorted_transcripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Porter Stemmer to Stem Words\n",
    "All text documents' words will be stemmed. Will also take a long time - do not run unless you do not have files with the appropriate ending. For example:\n",
    "\n",
    "FOMC19820202meetingStop.txt -> \n",
    "FOMC19820202meetingStopstemmed.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for doc in sorted_transcripts:\n",
    "#     !python porterstemmer.py {doc}\n",
    "#     i += 1\n",
    "#     print(\"File \" + str(i) + \" of \" + str(len(sorted_transcripts)) + \" Completed\")\n",
    "#     print(str(int(i * 100 / len(sorted_transcripts)))  + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now re-re-adjust our sorted_transcripts list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents 216\n"
     ]
    }
   ],
   "source": [
    "# run all the adjusting of variable sorted_transcripts in order\n",
    "mod_transcripts = []\n",
    "for link in sorted_transcripts:\n",
    "    mod_transcripts.append(str(link)[:-4] + \"stemmed.txt\")\n",
    "sorted_transcripts = mod_transcripts\n",
    "print(\"Number of Documents\", len(sorted_transcripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lists of Weighted Word Counts\n",
    "The following code will remove stop words and create a list of tfidf weighted word counts, in the order appearing in sorted_transcripts. It will also do the same thing with raw word stem counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "input_vectorizer = TfidfVectorizer(input=\"filename\", stop_words=None)\n",
    "m = input_vectorizer.fit_transform(sorted_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Word Stem Vectors: 17008\n",
      "Shape of Vector Matrix: (216, 17008)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Word Stem Vectors:\", m.shape[1])\n",
    "print(\"Shape of Vector Matrix:\", m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# same thing but with no global weighting\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "input_vectorizer_counts = CountVectorizer(input=\"filename\", stop_words=None)\n",
    "c = input_vectorizer_counts.fit_transform(sorted_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Word Stem Vectors: 17008\n",
      "Shape of Vector Matrix: (216, 17008)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Word Stem Vectors:\", c.shape[1])\n",
    "print(\"Shape of Vector Matrix:\", c.shape)\n",
    "# should be same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without stop words removed or stemming appied, we have 28917 word vectors, which each row representing a different data point and each value representing the number of occurences of the word.\n",
    "\n",
    "Stop words reduces the number by about 30. Stemming reduces the number to 17,008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append transcript paths and Weighted Word Counts to the table at the corresponding entry\n",
    "The following code adds the path of the fed transcript text data and the weighted word counts to the corresponding row in the df_outcome_var table. We place the transcript in a row if it took place before the meeting date for that row, but after the previous row's meeting date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended column at end\n",
      "appended column at end\n",
      "Should say 'appended column at end' only twice!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "transcript_path_col = []\n",
    "weighted_word_count_col = []\n",
    "word_count_col = []\n",
    "curr_date_row_counter = 0\n",
    "subgroup_transcript = []\n",
    "subgroup_wordcount = []\n",
    "subgroup_wordcount_c = []\n",
    "i = 0\n",
    "while i < len(sorted_transcripts):\n",
    "    f_month = str(int(df_outcome_vars.iloc[curr_date_row_counter, 1]))\n",
    "    f_day = str(int(df_outcome_vars.iloc[curr_date_row_counter, 2]))\n",
    "    f_year = str(int(df_outcome_vars.iloc[curr_date_row_counter, 0]))\n",
    "    fed_date = pd.to_datetime(f_month + \"/\" + f_day + \"/\" + f_year)\n",
    "    \n",
    "    link = sorted_transcripts[i]\n",
    "    date = link.rsplit('/', 1)[-1]\n",
    "    if date[0] == \"F\":\n",
    "        month = date[8:10]\n",
    "        day = date[10:12]\n",
    "        year = date[4:8]\n",
    "    else:\n",
    "        month = date[4:6]\n",
    "        day = date[6:8]\n",
    "        year = date[0:4]\n",
    "    text_date = pd.to_datetime(month + \"/\" + day + \"/\" + year)\n",
    "    if text_date <= fed_date:\n",
    "        subgroup_wordcount.append(m[i])\n",
    "        subgroup_transcript.append(link)\n",
    "        subgroup_wordcount_c.append(c[i])\n",
    "        i += 1\n",
    "    else:\n",
    "        transcript_path_col.append(subgroup_transcript)\n",
    "        weighted_word_count_col.append(subgroup_wordcount)\n",
    "        word_count_col.append(subgroup_wordcount_c)\n",
    "        subgroup_transcript = []\n",
    "        subgroup_wordcount = []\n",
    "        subgroup_wordcount_c = []\n",
    "        curr_date_row_counter += 1\n",
    "        \n",
    "transcript_path_col.append(subgroup_transcript)\n",
    "weighted_word_count_col.append(subgroup_wordcount)\n",
    "word_count_col.append(subgroup_wordcount_c)\n",
    "\n",
    "# append two empty lists representing columns\n",
    "while len(transcript_path_col) != df_outcome_vars.shape[0] and len(weighted_word_count_col) != df_outcome_vars.shape[0] and len(word_count_col) != df_outcome_vars.shape[0]:\n",
    "    print(\"appended column at end\")\n",
    "    transcript_path_col.append([])\n",
    "    weighted_word_count_col.append([]) \n",
    "    word_count_col.append([])\n",
    "print(\"Should say 'appended column at end' only twice!\")\n",
    "\n",
    "df_outcome_vars[\"Transcripts\"] = transcript_path_col\n",
    "df_outcome_vars[\"WeightedWordCount\"] = weighted_word_count_col\n",
    "df_outcome_vars[\"WordCount\"] = word_count_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append Relevant Words Count To Table\n",
    "We now select all vectors in list, average each vector, select relevant words, normalize such that the length of each vector is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a general look at data to decide which words seem to vary the most - would involve some table transformations\n",
    "relevant_words = ['recoveri', 'save', 'continu', 'expect', 'stock', 'profit', 'gain', 'fund', 'resili', 'household', 'indic', 'bear', 'distribut', 'custom', 'incom', 'bull', 'particip', 'oil', 'suppli', 'employ', 'confid', 'bank', 'forecast', 'price', 'foreign', 'tax', 'stagnat', 'headwind', 'debt', 'wage', 'growth', 'unemploy', 'workforc', 'weak', 'geopolit', 'dramat', 'demand', 'labor', 'consum', 'job', 'produc', 'risk', 'polici', 'strong', 'rate', 'global', 'energi', 'corpor', 'deficit', 'supplier', 'exchang', 'commod', 'wealth', 'inflat', 'condit', 'capit', 'market', 'inflationari', 'economi', 'abroad', 'mortgag', 'percent', 'lack', 'crisi', 'lend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "# first get the indices of the relevant words in our tokenCount arrays (for use below)\n",
    "feature_names = input_vectorizer.get_feature_names()\n",
    "rel_word_indices = [feature_names.index(word) for word in relevant_words]\n",
    "\n",
    "# create normalization function\n",
    "def norm(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    return v / norm\n",
    "\n",
    "# now begin process\n",
    "rel_word_count = []\n",
    "for lst in weighted_word_count_col:\n",
    "    if len(lst) == 0:\n",
    "        rel_word_count.append([])\n",
    "    else:\n",
    "        added_list = functools.reduce(np.add, lst)\n",
    "        averaged_list = added_list / len(lst)\n",
    "        rel_word_list = np.array([averaged_list.toarray()[0][i] for i in rel_word_indices])\n",
    "        normalized_rel_word_list = norm(rel_word_list) * 1000\n",
    "        rel_word_count.append(normalized_rel_word_list)\n",
    "\n",
    "df_outcome_vars[\"RelevantWordVector\"] = rel_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we choose reduce our table size such that we only consider *** changes *** in federal funds rate.\n",
    "\n",
    "We will train our model on a weighted average of the word vectors from the associated meeting and meeting prior\n",
    "\n",
    "We exclude data points with no transcripts associated with them (specifically, that meeting and the before the rate change), or single successive changes that do not have an associated transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get relevant data (transcripts from year of change and change before) from df for train and test\n",
    "# fed_rates = np.array(df_outcome_vars[\"Federal Funds Target Rate\"])\n",
    "change_rates = np.array(df_outcome_vars[\"Change in Fed Funds Target Rate\"])\n",
    "transcripts = np.array(df_outcome_vars[\"Transcripts\"])\n",
    "changes = []\n",
    "i = 0\n",
    "# while i < len(fed_rates) - 1:\n",
    "#     if fed_rates[i + 1] != fed_rates[i]:\n",
    "#         changes.append(True)\n",
    "#         changes.append(True)\n",
    "#         i += 2\n",
    "#     else:\n",
    "#         changes.append(False)\n",
    "#         i += 1\n",
    "\n",
    "# changes.append(False)\n",
    "\n",
    "changes.append(False)\n",
    "i += 1\n",
    "while i < len(change_rates) - 1:\n",
    "    if change_rates[i] != 0 and len(transcripts[i]) != 0:\n",
    "        changes.append(True)\n",
    "    else:\n",
    "        changes.append(False)\n",
    "    i += 1\n",
    "\n",
    "changes.append(False)\n",
    "\n",
    "relevant_data_df = df_outcome_vars[changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', len(relevant_data_df))\n",
    "# relevant_data_df.loc[:, [\"Year\", \"Month\", \"Day\", \"Transcripts\", \"Change in Fed Funds Target Rate\", \"RelevantWordVector\"]]\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reduction_df = relevant_data_df[relevant_data_df[\"Change in Fed Funds Target Rate\"] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "increase_df = relevant_data_df[relevant_data_df[\"Change in Fed Funds Target Rate\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training to Fit Model For Basis Point Change\n",
    "Here we will split our data into training and test sets. We will use our training data to construct a Lasso linear model using iterative fitting along the regularization path. \n",
    "\n",
    "We will do this for both a reduction and increase in basis point change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDUCTION\n",
      "Number of Samples 42\n",
      "number of train samples 28\n",
      "proportion of train samples 0.6666666666666666\n",
      "number of test samples 14\n"
     ]
    }
   ],
   "source": [
    "# split into test/train data.\n",
    "sample_num1 = reduction_df.shape[0]\n",
    "\n",
    "def random_bool(shape, p=0.5):\n",
    "    n = np.prod(shape)\n",
    "    x = np.fromstring(np.random.bytes(n), np.uint8, n)\n",
    "    return (x < 255 * p).reshape(shape)\n",
    "\n",
    "sample_gen1 = random_bool(sample_num1, 0.75)\n",
    "\n",
    "# see how many samples we're taking / what percentage is train data\n",
    "c = 0\n",
    "for b in sample_gen1:\n",
    "    if b:\n",
    "        c += 1\n",
    "        \n",
    "train_df_reduc = reduction_df[sample_gen1]\n",
    "test_df_reduc = reduction_df[np.invert(sample_gen1)]\n",
    "\n",
    "print(\"REDUCTION\")\n",
    "print(\"Number of Samples\", sample_num1)\n",
    "print(\"number of train samples\", c)\n",
    "print(\"proportion of train samples\", c / sample_num1)\n",
    "print(\"number of test samples\", sample_num1 - c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCREASE\n",
      "Number of Samples 47\n",
      "number of train samples 37\n",
      "proportion of train samples 0.7872340425531915\n",
      "number of test samples 10\n"
     ]
    }
   ],
   "source": [
    "sample_num2 = increase_df.shape[0]\n",
    "sample_gen2 = random_bool(sample_num2, 0.75)\n",
    "c = 0\n",
    "for b in sample_gen2:\n",
    "    if b:\n",
    "        c += 1\n",
    "\n",
    "train_df_increas = increase_df[sample_gen2]\n",
    "test_df_increas = increase_df[np.invert(sample_gen2)]\n",
    "\n",
    "print(\"INCREASE\")\n",
    "print(\"Number of Samples\", sample_num2)\n",
    "print(\"number of train samples\", c)\n",
    "print(\"proportion of train samples\", c / sample_num2)\n",
    "print(\"number of test samples\", sample_num2 - c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Reduction and Increase in Federal Funds Basis Point Change\n",
    "Now we find the coefficients to our matrix and output relevant graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train_reduc = np.array(train_df_reduc[\"Change in Fed Funds Target Rate\"])\n",
    "X_train_reduc = [list(x) for x in np.array(train_df_reduc[\"RelevantWordVector\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_reduc = np.array(test_df_reduc[\"Change in Fed Funds Target Rate\"])\n",
    "X_test_reduc = [list(x) for x in np.array(test_df_reduc[\"RelevantWordVector\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_increas = np.array(train_df_increas[\"Change in Fed Funds Target Rate\"])\n",
    "X_train_increas = [list(x) for x in np.array(train_df_increas[\"RelevantWordVector\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test_increas = np.array(test_df_increas[\"Change in Fed Funds Target Rate\"])\n",
    "X_test_increas = [list(x) for x in np.array(test_df_increas[\"RelevantWordVector\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00  -4.62863797e-04   0.00000000e+00   4.83390119e-04\n",
      "  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "  -5.82520796e-05  -0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   1.37783045e-04   0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "linear_clf = lm.LassoCV(tol=.001, cv=3)\n",
    "\n",
    "# Fit your classifier\n",
    "linear_clf.fit(X_train_reduc, Y_train_reduc)\n",
    "\n",
    "# Output Coefficients\n",
    "print(linear_clf.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 'recoveri')\n",
      "(0.0, 'save')\n",
      "(-0.0, 'continu')\n",
      "(-0.0, 'expect')\n",
      "(0.0, 'stock')\n",
      "(0.0, 'profit')\n",
      "(-0.0, 'gain')\n",
      "(0.0, 'fund')\n",
      "(-0.0, 'resili')\n",
      "(0.0, 'household')\n",
      "(0.0, 'indic')\n",
      "(-0.0, 'bear')\n",
      "(0.0, 'distribut')\n",
      "(-0.0, 'custom')\n",
      "(0.0, 'incom')\n",
      "(-0.0, 'bull')\n",
      "(-0.0, 'particip')\n",
      "(0.0, 'oil')\n",
      "(0.0, 'suppli')\n",
      "(0.0, 'employ')\n",
      "(-0.0, 'confid')\n",
      "(-0.00046286379737506603, 'bank')\n",
      "(0.0, 'forecast')\n",
      "(0.00048339011872315819, 'price')\n",
      "(-0.0, 'foreign')\n",
      "(-0.0, 'tax')\n",
      "(-0.0, 'stagnat')\n",
      "(-0.0, 'headwind')\n",
      "(-0.0, 'debt')\n",
      "(0.0, 'wage')\n",
      "(-0.0, 'growth')\n",
      "(-0.0, 'unemploy')\n",
      "(-0.0, 'workforc')\n",
      "(-0.0, 'weak')\n",
      "(0.0, 'geopolit')\n",
      "(0.0, 'dramat')\n",
      "(0.0, 'demand')\n",
      "(-0.0, 'labor')\n",
      "(-0.0, 'consum')\n",
      "(0.0, 'job')\n",
      "(0.0, 'produc')\n",
      "(-0.0, 'risk')\n",
      "(0.0, 'polici')\n",
      "(-0.0, 'strong')\n",
      "(-5.8252079552950643e-05, 'rate')\n",
      "(-0.0, 'global')\n",
      "(-0.0, 'energi')\n",
      "(0.0, 'corpor')\n",
      "(-0.0, 'deficit')\n",
      "(-0.0, 'supplier')\n",
      "(-0.0, 'exchang')\n",
      "(0.0, 'commod')\n",
      "(-0.0, 'wealth')\n",
      "(0.0, 'inflat')\n",
      "(-0.0, 'condit')\n",
      "(0.0, 'capit')\n",
      "(-0.0, 'market')\n",
      "(0.0, 'inflationari')\n",
      "(-0.0, 'economi')\n",
      "(-0.0, 'abroad')\n",
      "(-0.0, 'mortgag')\n",
      "(0.00013778304510949835, 'percent')\n",
      "(0.0, 'lack')\n",
      "(-0.0, 'crisi')\n",
      "(0.0, 'lend')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(linear_clf.coef_):\n",
    "    print((linear_clf.coef_[i], relevant_words[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE - Deviation between Actual and Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0.172953064708\n"
     ]
    }
   ],
   "source": [
    "# Output RMSE on test set\n",
    "def rmse(predicted_y, actual_y):\n",
    "    return np.sqrt(np.mean((predicted_y - actual_y) ** 2))\n",
    "\n",
    "print(\"RMSE\", rmse(linear_clf.predict(X_test_reduc), Y_test_reduc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.40129176, -0.36133812, -0.34368307, -0.30968473, -0.28815103,\n",
       "       -0.28232152, -0.31684374, -0.39301967, -0.28095028, -0.39420921,\n",
       "       -0.37633708, -0.37473606, -0.38259315, -0.24900028])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_clf.predict(X_test_reduc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5, -0.5, -0.25, -0.5, -0.25, -0.3125, -0.25, -0.25, -0.25, -0.25,\n",
       "       -0.5, -0.5, -0.5, -0.75], dtype=object)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_reduc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-stats + SD to measure significance of each coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_stats = []\n",
    "std = []\n",
    "coefficient_matrix = linear_clf.coef_\n",
    "i = 0\n",
    "while i < len(coefficient_matrix):\n",
    "    vals = []\n",
    "    for x_sample in X_train_reduc:\n",
    "        vals.append(x_sample[i])\n",
    "    standard_deviation = np.std(vals)\n",
    "    std.append(standard_deviation)\n",
    "    n = len(X_train_reduc)\n",
    "    standard_error = standard_deviation / np.sqrt(n)\n",
    "    t_stats.append((coefficient_matrix[i] - 0) / standard_error)\n",
    "    i += 1\n",
    "std = np.array(std)\n",
    "std= np.round(std, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -2.7468906470938074e-05, 0.0, 3.0845568656507008e-05, -0.0, -0.0, -0.0, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -2.6408435419382953e-06, -0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 4.4906557709323541e-06, 0.0, -0.0, 0.0]\n",
      "\n",
      "\n",
      "[  34.73   18.45   37.16   51.14   39.48   17.6    13.02   70.25    6.73\n",
      "   21.14   29.43    5.34   10.11    6.55   23.73    1.85   13.62   54.87\n",
      "   21.81   26.21   36.17   89.16   62.78   82.92   25.86   25.83    6.79\n",
      "   12.75   27.87   24.36   84.67   23.42    4.13   47.62    1.47    8.55\n",
      "   25.81   29.1    32.86   17.94   14.82   97.46   69.13   23.7   116.72\n",
      "   18.56   24.49   12.56   14.22    5.37   33.94   17.94   16.01  123.29\n",
      "   26.83   52.24  123.31   15.09   66.61   10.54   31.    162.35    5.25\n",
      "   15.37   21.85]\n",
      "35.2604615385\n"
     ]
    }
   ],
   "source": [
    "print(t_stats)\n",
    "print(\"\\n\")\n",
    "print(std)\n",
    "print(np.mean(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='ali-wetrill', api_key='x8ZULc5B5lFJLxnFCFD8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each basis point change - variation in number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basis_changes = sorted(list(set(list(reduction_df[\"Change in Fed Funds Target Rate\"]))))\n",
    "basis_changes.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vals = [[np.sum(x[0]) for x in list(reduction_df[reduction_df[\"Change in Fed Funds Target Rate\"] == j][\"WordCount\"])] for j in basis_changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(basis_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ali-wetrill/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', 'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)']\n",
    "\n",
    "trace0 = go.Box(\n",
    "    y = vals[0],\n",
    "    name = str(basis_changes[0]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[0]),\n",
    "    line = dict(\n",
    "        color = colors[0])\n",
    ")\n",
    "\n",
    "trace1 = go.Box(\n",
    "    y = vals[1],\n",
    "    name = str(basis_changes[1]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[1]),\n",
    "    line = dict(\n",
    "        color = colors[1])\n",
    ")\n",
    "\n",
    "trace2 = go.Box(\n",
    "    y = vals[2],\n",
    "    name = str(basis_changes[2]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[2]),\n",
    "    line = dict(\n",
    "        color = colors[2])\n",
    ")\n",
    "\n",
    "trace3 = go.Box(\n",
    "    y = vals[3],\n",
    "    name = str(basis_changes[3]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[3]),\n",
    "    line = dict(\n",
    "        color = colors[3])\n",
    ")\n",
    "\n",
    "trace4 = go.Box(\n",
    "    y = vals[4],\n",
    "    name = str(basis_changes[4]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[4]),\n",
    "    line = dict(\n",
    "        color = colors[4])\n",
    ")\n",
    "\n",
    "trace5 = go.Box(\n",
    "    y = vals[5],\n",
    "    name = str(basis_changes[5]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[5]),\n",
    "    line = dict(\n",
    "        color = colors[5])\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3,trace4,trace5]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Variation in Words within each Transcript\",\n",
    "    xaxis=dict(\n",
    "        type=\"category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig, filename = \"Box Plot Styling Outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each basis point change - top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count_per_basis_change = []\n",
    "for rate in basis_changes:\n",
    "    z = list(reduction_df[reduction_df[\"Change in Fed Funds Target Rate\"] == rate][\"WordCount\"])\n",
    "    word_count_per_basis_change.append([sum([x[0].toarray()[0][i] for x in z]) for i in range(0, len(z[0][0].toarray()[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_word_count = [sum(word_count_per_basis_change[i]) for i in range(0, len(word_count_per_basis_change))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_word_counts = [np.array(word_count_per_basis_change[i]) / total_word_count[i] for i in range(0, len(total_word_count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_word_counts_with_words = []\n",
    "for arr in percent_word_counts:\n",
    "    i = 0\n",
    "    new_arr = []\n",
    "    names = input_vectorizer_counts.get_feature_names()\n",
    "    while i < len(arr):\n",
    "        new_arr.append((arr[i], names[i]))\n",
    "        i += 1\n",
    "    percent_word_counts_with_words.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percent_word_counts_with_words = [sorted(list(x)) for x in percent_word_counts_with_words]\n",
    "dummy = [x.reverse() for x in percent_word_counts_with_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual inspetion. adjust i for starting basis_changes. j for within a basis_change.\n",
    "# i = 2\n",
    "# while i < len(percent_word_counts_with_words):\n",
    "#     print(basis_changes[i])\n",
    "#     j = 43\n",
    "#     while j < 50:\n",
    "#         print(percent_word_counts_with_words[i][j])\n",
    "#         j += 1\n",
    "#     print(\"\\n\")\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manual inspection on which to plot\n",
    "ind_2_25 = [5, 12, 13, 14, 15, 16, 26, 34, 43, 44]\n",
    "ind_4_05 = [3, 5, 13, 14, 15, 18, 22, 25, 26, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25\n",
      "0.715% market\n",
      "0.527% growth\n",
      "0.518% percent\n",
      "0.491% inflat\n",
      "0.479% price\n",
      "0.428% polici\n",
      "0.383% risk\n",
      "0.313% fund\n",
      "0.27% bank\n",
      "0.265% continu\n",
      "\n",
      "\n",
      "-0.5\n",
      "1.037% rate\n",
      "0.761% market\n",
      "0.469% percent\n",
      "0.404% risk\n",
      "0.402% growth\n",
      "0.391% economi\n",
      "0.379% polici\n",
      "0.365% expect\n",
      "0.359% forecast\n",
      "0.35% bank\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(basis_changes[i])\n",
    "for j in ind_2_25:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 4\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_4_05:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A Closer Look at Relevant Words\n",
    "Percentages for -0.5 and -0.25 changes. And how the most relevant change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "j = 0\n",
    "ind_relwords_25 = []\n",
    "while j < len(percent_word_counts_with_words[i]):\n",
    "    if percent_word_counts_with_words[i][j][1] in relevant_words:\n",
    "        ind_relwords_25.append(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "j = 0\n",
    "ind_relwords_05 = []\n",
    "while j < len(percent_word_counts_with_words[i]):\n",
    "    if percent_word_counts_with_words[i][j][1] in relevant_words:\n",
    "        ind_relwords_05.append(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25\n",
      "1.054% rate\n",
      "0.715% market\n",
      "0.527% growth\n",
      "0.518% percent\n",
      "0.491% inflat\n",
      "0.479% price\n",
      "0.428% polici\n",
      "0.419% economi\n",
      "0.411% expect\n",
      "0.405% forecast\n",
      "0.383% risk\n",
      "0.313% fund\n",
      "0.27% bank\n",
      "0.265% continu\n",
      "0.154% weak\n",
      "0.149% indic\n",
      "0.148% capit\n",
      "0.142% demand\n",
      "0.126% consum\n",
      "0.126% condit\n",
      "0.101% strong\n",
      "0.092% confid\n",
      "0.09% employ\n",
      "0.084% incom\n",
      "0.084% labor\n",
      "0.083% stock\n",
      "0.077% unemploy\n",
      "0.076% oil\n",
      "0.071% foreign\n",
      "0.067% suppli\n",
      "0.067% exchang\n",
      "0.064% tax\n",
      "0.06% debt\n",
      "0.058% energi\n",
      "0.056% job\n",
      "0.054% produc\n",
      "0.051% household\n",
      "0.049% mortgag\n",
      "0.047% wage\n",
      "0.047% recoveri\n",
      "0.042% profit\n",
      "0.042% inflationari\n",
      "0.041% particip\n",
      "0.038% lend\n",
      "0.038% commod\n",
      "0.037% gain\n",
      "0.036% corpor\n",
      "0.028% save\n",
      "0.026% dramat\n",
      "0.025% distribut\n",
      "0.021% deficit\n",
      "0.019% global\n",
      "0.018% wealth\n",
      "0.018% abroad\n",
      "0.016% lack\n",
      "0.01% custom\n",
      "0.01% crisi\n",
      "0.008% resili\n",
      "0.008% bear\n",
      "0.004% headwind\n",
      "0.004% supplier\n",
      "0.002% workforc\n",
      "0.001% geopolit\n",
      "0.0% stagnat\n",
      "0.0% bull\n",
      "\n",
      "\n",
      "-0.5\n",
      "1.037% rate\n",
      "0.761% market\n",
      "0.469% percent\n",
      "0.404% risk\n",
      "0.402% growth\n",
      "0.391% economi\n",
      "0.379% polici\n",
      "0.365% expect\n",
      "0.359% forecast\n",
      "0.35% bank\n",
      "0.341% fund\n",
      "0.308% inflat\n",
      "0.279% price\n",
      "0.221% continu\n",
      "0.175% weak\n",
      "0.161% consum\n",
      "0.143% indic\n",
      "0.142% demand\n",
      "0.129% condit\n",
      "0.121% confid\n",
      "0.12% capit\n",
      "0.099% strong\n",
      "0.09% foreign\n",
      "0.083% unemploy\n",
      "0.077% stock\n",
      "0.075% labor\n",
      "0.074% employ\n",
      "0.07% incom\n",
      "0.066% debt\n",
      "0.06% recoveri\n",
      "0.06% exchang\n",
      "0.057% mortgag\n",
      "0.052% suppli\n",
      "0.047% particip\n",
      "0.046% lend\n",
      "0.044% tax\n",
      "0.043% job\n",
      "0.043% energi\n",
      "0.04% produc\n",
      "0.04% corpor\n",
      "0.037% household\n",
      "0.036% oil\n",
      "0.032% dramat\n",
      "0.031% global\n",
      "0.026% wealth\n",
      "0.025% save\n",
      "0.024% gain\n",
      "0.024% deficit\n",
      "0.023% inflationari\n",
      "0.023% abroad\n",
      "0.023% profit\n",
      "0.022% crisi\n",
      "0.018% commod\n",
      "0.017% wage\n",
      "0.016% distribut\n",
      "0.015% custom\n",
      "0.014% lack\n",
      "0.011% bear\n",
      "0.009% headwind\n",
      "0.009% resili\n",
      "0.006% supplier\n",
      "0.003% geopolit\n",
      "0.002% stagnat\n",
      "0.002% workforc\n",
      "0.0% bull\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(basis_changes[i])\n",
    "for j in ind_relwords_25:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 4\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_relwords_05:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot how they change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# top word counts in every date\n",
    "# sum of all words in every date for neg changesn\n",
    "# list of lists, with each inner list holding a particular word frequency change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts_with_words = []\n",
    "z = 1\n",
    "for arr in list(reduction_df[\"WordCount\"]):\n",
    "    lst = arr[0].toarray()[0]\n",
    "    s = sum(lst)\n",
    "    i = 0\n",
    "    new_arr = []\n",
    "    names = input_vectorizer_counts.get_feature_names()\n",
    "    while i < len(lst):\n",
    "        new_arr.append((lst[i] * 100 / s, names[i]))\n",
    "        i += 1\n",
    "    word_counts_with_words.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts_with_words = [sorted(list(x)) for x in word_counts_with_words]\n",
    "dummy = [x.reverse() for x in word_counts_with_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lst_ind_relwords = []\n",
    "# i = 0\n",
    "# while i < len(word_counts_with_words):\n",
    "#     j = 0\n",
    "#     ind_relwords = []\n",
    "#     while j < len(word_counts_with_words[i]):\n",
    "#         if word_counts_with_words[i][j][1] in relevant_words:\n",
    "#             ind_relwords.append(j)\n",
    "#         j += 1\n",
    "#     lst_ind_relwords.append(ind_relwords)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_word_counts_with_words = []\n",
    "i = 0\n",
    "while i < len(word_counts_with_words):\n",
    "    lst = word_counts_with_words[i]\n",
    "    new_lst = []\n",
    "    j = 0\n",
    "    while j < len(lst):\n",
    "        if lst[j][1] in relevant_words:\n",
    "            new_lst.append(lst[j])\n",
    "        j += 1\n",
    "    rel_word_counts_with_words.append(new_lst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findItem(theList, word):\n",
    "    return [theList.index(i) for i in theList if i[1] == word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rel_word_g2 = [[findItem(x, word) for x in rel_word_counts_with_words] for word in relevant_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recoveri'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevant Lists\n",
    "# rel_word_counts_with_words - word counts for each date. In each list use rel_word_g2 to get word count.\n",
    "# rel_word_g2 - for each index, lst is where that word is in the above list.\n",
    "# relevant_words - word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.076523640338890406, 'recoveri')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_word_counts_with_words[0][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_vals = []\n",
    "for word_indices in rel_word_g2:\n",
    "    i = 0\n",
    "    sub_list = []\n",
    "    while i < len(rel_word_counts_with_words):\n",
    "        index = word_indices[i]\n",
    "        date_lst = rel_word_counts_with_words[i]\n",
    "        sub_list.append(date_lst[index])\n",
    "        i += 1\n",
    "    y_vals.append(sub_list)\n",
    "\n",
    "labels = relevant_words\n",
    "\n",
    "yr = list(reduction_df[\"Year\"])\n",
    "mnth = list(reduction_df[\"Month\"])\n",
    "day = list(reduction_df[\"Day\"])\n",
    "x_vals = []\n",
    "for i in zip(mnth, day, yr):\n",
    "    date = pd.to_datetime(str(i[0]) + \"/\" + str(i[1]) + \"/\" + str(i[2]))\n",
    "    x_vals.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ali-wetrill/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[0]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[0]\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[1]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[1]\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[2]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[2]\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[3]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[3]\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[4]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[4]\n",
    ")\n",
    "\n",
    "trace5 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[5]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[5]\n",
    ")\n",
    "\n",
    "trace6 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[6]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[6]\n",
    ")\n",
    "\n",
    "trace7 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[7]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[7]\n",
    ")\n",
    "\n",
    "trace8 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[8]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[8]\n",
    ")\n",
    "\n",
    "trace9 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[9]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[9]\n",
    ")\n",
    "\n",
    "trace10 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[10]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[10]\n",
    ")\n",
    "\n",
    "trace11 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[11]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[11]\n",
    ")\n",
    "\n",
    "trace12 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[12]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[12]\n",
    ")\n",
    "\n",
    "trace13 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[13]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[13]\n",
    ")\n",
    "\n",
    "trace14 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[14]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[14]\n",
    ")\n",
    "\n",
    "trace15 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[15]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[15]\n",
    ")\n",
    "\n",
    "trace16 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[16]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[16]\n",
    ")\n",
    "\n",
    "trace17 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[17]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[17]\n",
    ")\n",
    "\n",
    "trace18 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[18]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[18]\n",
    ")\n",
    "\n",
    "trace19 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[19]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[19]\n",
    ")\n",
    "\n",
    "trace20 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[20]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[20]\n",
    ")\n",
    "\n",
    "trace21 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[21]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[21]\n",
    ")\n",
    "\n",
    "trace22 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[22]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[22]\n",
    ")\n",
    "\n",
    "trace23 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[23]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[23]\n",
    ")\n",
    "\n",
    "trace24 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[24]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[24]\n",
    ")\n",
    "\n",
    "trace25 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[25]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[25]\n",
    ")\n",
    "\n",
    "trace26 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[26]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[26]\n",
    ")\n",
    "\n",
    "trace27 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[27]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[27]\n",
    ")\n",
    "\n",
    "trace28 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[28]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[28]\n",
    ")\n",
    "\n",
    "trace29 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[29]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[29]\n",
    ")\n",
    "\n",
    "trace30 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[30]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[30]\n",
    ")\n",
    "\n",
    "trace31 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[31]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[31]\n",
    ")\n",
    "\n",
    "trace32 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[32]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[32]\n",
    ")\n",
    "\n",
    "trace33 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[33]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[33]\n",
    ")\n",
    "\n",
    "trace34 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[34]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[34]\n",
    ")\n",
    "\n",
    "trace35 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[35]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[35]\n",
    ")\n",
    "\n",
    "trace36 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[36]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[36]\n",
    ")\n",
    "\n",
    "trace37 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[37]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[37]\n",
    ")\n",
    "\n",
    "trace38 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[38]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[38]\n",
    ")\n",
    "\n",
    "trace39 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[39]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[39]\n",
    ")\n",
    "\n",
    "trace40 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[40]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[40]\n",
    ")\n",
    "\n",
    "trace41 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[41]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[41]\n",
    ")\n",
    "\n",
    "trace42 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[42]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[42]\n",
    ")\n",
    "\n",
    "trace43 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[43]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[43]\n",
    ")\n",
    "\n",
    "trace44 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[44]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[44]\n",
    ")\n",
    "\n",
    "trace45 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[45]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[45]\n",
    ")\n",
    "\n",
    "trace46 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[46]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[46]\n",
    ")\n",
    "\n",
    "trace47 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[47]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[47]\n",
    ")\n",
    "\n",
    "trace48 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[48]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[48]\n",
    ")\n",
    "\n",
    "trace49 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[49]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[49]\n",
    ")\n",
    "\n",
    "trace50 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[50]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[50]\n",
    ")\n",
    "\n",
    "trace51 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[51]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[51]\n",
    ")\n",
    "\n",
    "trace52 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[52]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[52]\n",
    ")\n",
    "\n",
    "trace53 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[53]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[53]\n",
    ")\n",
    "\n",
    "trace54 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[54]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[54]\n",
    ")\n",
    "\n",
    "trace55 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[55]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[55]\n",
    ")\n",
    "\n",
    "trace56 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[56]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[56]\n",
    ")\n",
    "\n",
    "trace57 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[57]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[57]\n",
    ")\n",
    "\n",
    "trace58 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[58]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[58]\n",
    ")\n",
    "\n",
    "trace59 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[59]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[59]\n",
    ")\n",
    "\n",
    "trace60 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[60]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[60]\n",
    ")\n",
    "\n",
    "trace61 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[61]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[61]\n",
    ")\n",
    "\n",
    "trace62 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[62]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[62]\n",
    ")\n",
    "\n",
    "trace63 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[63]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[63]\n",
    ")\n",
    "\n",
    "trace64 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[64]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[64]\n",
    ")\n",
    "\n",
    "data = [\n",
    "       trace2, trace3, trace7, \n",
    "       trace21, trace22, trace23, \n",
    "       trace30,\n",
    "       trace41, trace42,\n",
    "       trace53\n",
    "]\n",
    "\n",
    "# data = [trace0, trace1, trace2, trace3, trace4, trace5, trace6, trace7, trace8, trace9, \n",
    "#         trace10, trace11, trace12, trace13, trace14, trace15, trace16, trace17, trace18, trace19,\n",
    "#        trace20, trace21, trace22, trace23, trace24, trace25, trace26, trace27, trace28, trace29, \n",
    "#        trace30, trace31, trace32, trace33, trace34, trace35, trace36, trace37, trace38, trace39,\n",
    "#        trace40, trace41, trace42, trace43, trace44, trace45, trace46, trace47, trace48, trace49,\n",
    "#        trace50, trace51, trace52, trace53, trace54, trace55, trace56, trace57, trace58, trace59,\n",
    "#        trace60, trace61, trace62, trace63, trace64]\n",
    "\n",
    "py.iplot(data, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relevant_words.index(\"economi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ind = [2, 3, 7, 21, 22, 23, 30, 41, 42, 44, 53, 56, 58, 61]\n",
    "# avg = []\n",
    "# for i in ind:\n",
    "#     avg.append(np.mean([y[0] for y in y_vals[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.00000000e+00  -0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   7.76648264e-05   0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00  -0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00   0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00   2.62593907e-06   0.00000000e+00  -0.00000000e+00\n",
      "   0.00000000e+00  -0.00000000e+00   0.00000000e+00  -0.00000000e+00\n",
      "  -0.00000000e+00  -5.54384077e-05   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model as lm\n",
    "\n",
    "linear_clf2 = lm.LassoCV(tol=.001, cv=3)\n",
    "\n",
    "# Fit your classifier\n",
    "linear_clf2.fit(X_train_increas, Y_train_increas)\n",
    "\n",
    "# Output Coefficients\n",
    "print(linear_clf2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.0, 'recoveri')\n",
      "(-0.0, 'save')\n",
      "(0.0, 'continu')\n",
      "(0.0, 'expect')\n",
      "(0.0, 'stock')\n",
      "(0.0, 'profit')\n",
      "(0.0, 'gain')\n",
      "(-0.0, 'fund')\n",
      "(-0.0, 'resili')\n",
      "(0.0, 'household')\n",
      "(0.0, 'indic')\n",
      "(0.0, 'bear')\n",
      "(0.0, 'distribut')\n",
      "(0.0, 'custom')\n",
      "(0.0, 'incom')\n",
      "(0.0, 'bull')\n",
      "(0.0, 'particip')\n",
      "(-0.0, 'oil')\n",
      "(-0.0, 'suppli')\n",
      "(0.0, 'employ')\n",
      "(0.0, 'confid')\n",
      "(0.0, 'bank')\n",
      "(-0.0, 'forecast')\n",
      "(0.0, 'price')\n",
      "(0.0, 'foreign')\n",
      "(-0.0, 'tax')\n",
      "(0.0, 'stagnat')\n",
      "(0.0, 'headwind')\n",
      "(-0.0, 'debt')\n",
      "(0.0, 'wage')\n",
      "(0.0, 'growth')\n",
      "(0.0, 'unemploy')\n",
      "(0.0, 'workforc')\n",
      "(-0.0, 'weak')\n",
      "(0.0, 'geopolit')\n",
      "(-0.0, 'dramat')\n",
      "(-0.0, 'demand')\n",
      "(0.0, 'labor')\n",
      "(0.0, 'consum')\n",
      "(0.0, 'job')\n",
      "(-0.0, 'produc')\n",
      "(0.0, 'risk')\n",
      "(7.7664826431034697e-05, 'polici')\n",
      "(0.0, 'strong')\n",
      "(-0.0, 'rate')\n",
      "(0.0, 'global')\n",
      "(-0.0, 'energi')\n",
      "(-0.0, 'corpor')\n",
      "(-0.0, 'deficit')\n",
      "(0.0, 'supplier')\n",
      "(0.0, 'exchang')\n",
      "(-0.0, 'commod')\n",
      "(0.0, 'wealth')\n",
      "(2.6259390725761357e-06, 'inflat')\n",
      "(0.0, 'condit')\n",
      "(-0.0, 'capit')\n",
      "(0.0, 'market')\n",
      "(-0.0, 'inflationari')\n",
      "(0.0, 'economi')\n",
      "(-0.0, 'abroad')\n",
      "(-0.0, 'mortgag')\n",
      "(-5.5438407719272833e-05, 'percent')\n",
      "(0.0, 'lack')\n",
      "(0.0, 'crisi')\n",
      "(0.0, 'lend')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < len(linear_clf2.coef_):\n",
    "    print((linear_clf2.coef_[i], relevant_words[i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 0.316532429258\n"
     ]
    }
   ],
   "source": [
    "# Output RMSE on test set\n",
    "def rmse(predicted_y, actual_y):\n",
    "    return np.sqrt(np.mean((predicted_y - actual_y) ** 2))\n",
    "\n",
    "print(\"RMSE\", rmse(linear_clf2.predict(X_test_increas), Y_test_increas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.21774323,  0.20283646,  0.23914756,  0.23025074,  0.23742124,\n",
       "        0.23821036,  0.2415549 ,  0.24697556,  0.24669141,  0.24860456])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_clf2.predict(X_test_increas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.125, 0.25, 0.125, 0.0625, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.25,\n",
       "       0.25, 0.25, 0.25, 0.25, 0.25, 0.25], dtype=object)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_increas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_stats = []\n",
    "std = []\n",
    "coefficient_matrix = linear_clf2.coef_\n",
    "i = 0\n",
    "while i < len(coefficient_matrix):\n",
    "    vals = []\n",
    "    for x_sample in X_train_increas:\n",
    "        vals.append(x_sample[i])\n",
    "    standard_deviation = np.std(vals)\n",
    "    std.append(standard_deviation)\n",
    "    n = len(X_train_increas)\n",
    "    standard_error = standard_deviation / np.sqrt(n)\n",
    "    t_stats.append((coefficient_matrix[i] - 0) / standard_error)\n",
    "    i += 1\n",
    "std = np.array(std)\n",
    "std= np.round(std, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 6.2124091116642235e-06, 0.0, -0.0, 0.0, -0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0, 1.2167800130819637e-07, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, -2.1649225161280406e-06, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "[  45.32   12.91   48.07   58.73   24.87   12.09   14.45   73.04    5.59\n",
      "   17.32   24.51    4.24    9.14    6.54   31.36    4.02   24.     52.39\n",
      "   30.62   38.44   16.49   47.94   64.64  136.28   30.57   15.2     1.68\n",
      "    6.71   29.03   32.31   63.31   19.76    5.01   19.41    6.34    7.39\n",
      "   37.78   38.62   25.35   19.02   16.09   48.04   76.04   26.44  135.04\n",
      "   22.83   62.19   10.58   23.32    6.78   34.93   17.72   14.77  131.27\n",
      "   14.19   18.52   85.13   22.63   46.25    8.96   23.28  155.76    5.14\n",
      "    7.27    9.18]\n",
      "33.5821538462\n"
     ]
    }
   ],
   "source": [
    "print(t_stats)\n",
    "print(\"\\n\")\n",
    "print(std)\n",
    "print(np.mean(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basis_changes = sorted(list(set(list(increase_df[\"Change in Fed Funds Target Rate\"]))))\n",
    "basis_changes.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.125, 0.75, 0.5, 0.3125, 0.25, 0.125, 0.0625]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basis_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals = [[np.sum(x[0]) for x in list(increase_df[increase_df[\"Change in Fed Funds Target Rate\"] == j][\"WordCount\"])] for j in basis_changes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ali-wetrill/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "colors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)', 'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', 'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)']\n",
    "\n",
    "trace0 = go.Box(\n",
    "    y = vals[0],\n",
    "    name = str(basis_changes[0]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[0]),\n",
    "    line = dict(\n",
    "        color = colors[0])\n",
    ")\n",
    "\n",
    "trace1 = go.Box(\n",
    "    y = vals[1],\n",
    "    name = str(basis_changes[1]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[1]),\n",
    "    line = dict(\n",
    "        color = colors[1])\n",
    ")\n",
    "\n",
    "trace2 = go.Box(\n",
    "    y = vals[2],\n",
    "    name = str(basis_changes[2]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[2]),\n",
    "    line = dict(\n",
    "        color = colors[2])\n",
    ")\n",
    "\n",
    "trace3 = go.Box(\n",
    "    y = vals[3],\n",
    "    name = str(basis_changes[3]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[3]),\n",
    "    line = dict(\n",
    "        color = colors[3])\n",
    ")\n",
    "\n",
    "trace4 = go.Box(\n",
    "    y = vals[4],\n",
    "    name = str(basis_changes[4]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[4]),\n",
    "    line = dict(\n",
    "        color = colors[4])\n",
    ")\n",
    "\n",
    "trace5 = go.Box(\n",
    "    y = vals[5],\n",
    "    name = str(basis_changes[5]),\n",
    "    jitter = 0.3,\n",
    "    pointpos = -1.8,\n",
    "    boxpoints = 'all',\n",
    "    marker = dict(\n",
    "        color = colors[5]),\n",
    "    line = dict(\n",
    "        color = colors[5])\n",
    ")\n",
    "\n",
    "data = [trace0,trace1,trace2,trace3,trace4,trace5]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = \"Variation in Words within each Transcript\",\n",
    "    xaxis=dict(\n",
    "        type=\"category\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig, filename = \"Box Plot Styling Outliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_count_per_basis_change = []\n",
    "for rate in basis_changes:\n",
    "    z = list(increase_df[increase_df[\"Change in Fed Funds Target Rate\"] == rate][\"WordCount\"])\n",
    "    word_count_per_basis_change.append([sum([x[0].toarray()[0][i] for x in z]) for i in range(0, len(z[0][0].toarray()[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_word_count = [sum(word_count_per_basis_change[i]) for i in range(0, len(word_count_per_basis_change))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percent_word_counts = [np.array(word_count_per_basis_change[i]) / total_word_count[i] for i in range(0, len(total_word_count))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percent_word_counts_with_words = []\n",
    "for arr in percent_word_counts:\n",
    "    i = 0\n",
    "    new_arr = []\n",
    "    names = input_vectorizer_counts.get_feature_names()\n",
    "    while i < len(arr):\n",
    "        new_arr.append((arr[i], names[i]))\n",
    "        i += 1\n",
    "    percent_word_counts_with_words.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percent_word_counts_with_words = [sorted(list(x)) for x in percent_word_counts_with_words]\n",
    "dummy = [x.reverse() for x in percent_word_counts_with_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual inspection. adjust i for starting basis_changes. j for within a basis_change.\n",
    "# i = 5\n",
    "# while i < len(percent_word_counts_with_words):\n",
    "#     print(basis_changes[i])\n",
    "#     j = 33\n",
    "#     while j < 50:\n",
    "#         print(percent_word_counts_with_words[i][j])\n",
    "#         j += 1\n",
    "#     print(\"\\n\")\n",
    "#     break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#manual inspection on which to plot\n",
    "ind_2_05 = [4, 6, 10, 14, 17, 18, 19, 21, 27, 29]\n",
    "ind_4_25 = [2, 5, 6, 7, 8, 9, 11, 16, 17, 18]\n",
    "ind_5_125 = [6, 17, 21, 24, 27, 28, 30, 32, 33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.865% rate\n",
      "0.661% inflat\n",
      "0.612% market\n",
      "0.501% polici\n",
      "0.428% increas\n",
      "0.423% growth\n",
      "0.387% time\n",
      "0.371% price\n",
      "0.337% economi\n",
      "0.321% committe\n",
      "\n",
      "\n",
      "0.25\n",
      "0.961% rate\n",
      "0.838% price\n",
      "0.778% inflat\n",
      "0.713% market\n",
      "0.708% year\n",
      "0.599% percent\n",
      "0.568% growth\n",
      "0.48% expect\n",
      "0.452% polici\n",
      "0.445% increas\n",
      "\n",
      "\n",
      "0.125\n",
      "0.996% rate\n",
      "0.5% growth\n",
      "0.413% market\n",
      "0.385% time\n",
      "0.375% forecast\n",
      "0.371% increas\n",
      "0.36% rang\n",
      "0.351% interest\n",
      "0.348% price\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(basis_changes[i])\n",
    "for j in ind_2_05:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 4\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_4_25:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 5\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_5_125:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a closer look at relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "j = 0\n",
    "ind_relwords_05 = []\n",
    "while j < len(percent_word_counts_with_words[i]):\n",
    "    if percent_word_counts_with_words[i][j][1] in relevant_words:\n",
    "        ind_relwords_05.append(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 4\n",
    "j = 0\n",
    "ind_relwords_25 = []\n",
    "while j < len(percent_word_counts_with_words[i]):\n",
    "    if percent_word_counts_with_words[i][j][1] in relevant_words:\n",
    "        ind_relwords_25.append(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "j = 0\n",
    "ind_relwords_125 = []\n",
    "while j < len(percent_word_counts_with_words[i]):\n",
    "    if percent_word_counts_with_words[i][j][1] in relevant_words:\n",
    "        ind_relwords_125.append(j)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.865% rate\n",
      "0.661% inflat\n",
      "0.612% market\n",
      "0.55% percent\n",
      "0.501% polici\n",
      "0.423% growth\n",
      "0.371% price\n",
      "0.337% economi\n",
      "0.301% expect\n",
      "0.293% forecast\n",
      "0.293% continu\n",
      "0.266% bank\n",
      "0.258% risk\n",
      "0.213% fund\n",
      "0.179% strong\n",
      "0.155% demand\n",
      "0.132% foreign\n",
      "0.128% indic\n",
      "0.127% employ\n",
      "0.123% labor\n",
      "0.108% exchang\n",
      "0.103% consum\n",
      "0.094% wage\n",
      "0.094% unemploy\n",
      "0.072% capit\n",
      "0.071% incom\n",
      "0.069% suppli\n",
      "0.068% condit\n",
      "0.065% stock\n",
      "0.065% job\n",
      "0.06% gain\n",
      "0.057% weak\n",
      "0.055% produc\n",
      "0.053% debt\n",
      "0.047% oil\n",
      "0.047% confid\n",
      "0.04% particip\n",
      "0.037% household\n",
      "0.033% inflationari\n",
      "0.029% tax\n",
      "0.026% deficit\n",
      "0.025% wealth\n",
      "0.021% recoveri\n",
      "0.021% dramat\n",
      "0.02% mortgag\n",
      "0.019% distribut\n",
      "0.015% profit\n",
      "0.015% lend\n",
      "0.015% corpor\n",
      "0.014% energi\n",
      "0.014% crisi\n",
      "0.013% lack\n",
      "0.013% custom\n",
      "0.013% abroad\n",
      "0.011% save\n",
      "0.01% supplier\n",
      "0.005% global\n",
      "0.005% commod\n",
      "0.003% resili\n",
      "0.003% bear\n",
      "0.001% bull\n",
      "0.0% workforc\n",
      "0.0% stagnat\n",
      "0.0% headwind\n",
      "0.0% geopolit\n",
      "\n",
      "\n",
      "0.25\n",
      "0.961% rate\n",
      "0.838% price\n",
      "0.778% inflat\n",
      "0.713% market\n",
      "0.599% percent\n",
      "0.568% growth\n",
      "0.48% expect\n",
      "0.452% polici\n",
      "0.385% forecast\n",
      "0.324% economi\n",
      "0.307% risk\n",
      "0.3% continu\n",
      "0.254% fund\n",
      "0.192% demand\n",
      "0.171% labor\n",
      "0.151% bank\n",
      "0.149% energi\n",
      "0.147% strong\n",
      "0.147% indic\n",
      "0.143% oil\n",
      "0.116% incom\n",
      "0.113% employ\n",
      "0.107% consum\n",
      "0.099% suppli\n",
      "0.093% condit\n",
      "0.087% foreign\n",
      "0.08% unemploy\n",
      "0.079% capit\n",
      "0.077% confid\n",
      "0.073% particip\n",
      "0.072% wage\n",
      "0.066% stock\n",
      "0.065% exchang\n",
      "0.065% job\n",
      "0.061% gain\n",
      "0.058% deficit\n",
      "0.058% debt\n",
      "0.055% produc\n",
      "0.054% mortgag\n",
      "0.052% global\n",
      "0.05% household\n",
      "0.048% inflationari\n",
      "0.046% weak\n",
      "0.045% commod\n",
      "0.044% save\n",
      "0.042% profit\n",
      "0.039% corpor\n",
      "0.034% recoveri\n",
      "0.034% tax\n",
      "0.028% wealth\n",
      "0.028% abroad\n",
      "0.026% distribut\n",
      "0.02% custom\n",
      "0.017% dramat\n",
      "0.014% lend\n",
      "0.012% lack\n",
      "0.011% bear\n",
      "0.01% supplier\n",
      "0.007% crisi\n",
      "0.006% resili\n",
      "0.003% workforc\n",
      "0.003% geopolit\n",
      "0.002% headwind\n",
      "0.001% bull\n",
      "0.0% stagnat\n",
      "\n",
      "\n",
      "0.125\n",
      "0.996% rate\n",
      "0.887% percent\n",
      "0.5% growth\n",
      "0.413% market\n",
      "0.375% forecast\n",
      "0.348% price\n",
      "0.295% economi\n",
      "0.26% inflat\n",
      "0.203% expect\n",
      "0.194% fund\n",
      "0.188% polici\n",
      "0.173% bank\n",
      "0.166% continu\n",
      "0.15% risk\n",
      "0.14% recoveri\n",
      "0.128% strong\n",
      "0.107% indic\n",
      "0.092% foreign\n",
      "0.08% demand\n",
      "0.077% exchang\n",
      "0.074% debt\n",
      "0.071% capit\n",
      "0.067% condit\n",
      "0.066% oil\n",
      "0.066% deficit\n",
      "0.064% consum\n",
      "0.062% labor\n",
      "0.062% confid\n",
      "0.061% produc\n",
      "0.058% weak\n",
      "0.054% inflationari\n",
      "0.05% unemploy\n",
      "0.049% energi\n",
      "0.047% employ\n",
      "0.04% suppli\n",
      "0.04% incom\n",
      "0.039% wage\n",
      "0.034% tax\n",
      "0.032% commod\n",
      "0.03% save\n",
      "0.026% gain\n",
      "0.024% abroad\n",
      "0.022% profit\n",
      "0.021% stock\n",
      "0.021% lend\n",
      "0.019% mortgag\n",
      "0.019% job\n",
      "0.017% dramat\n",
      "0.013% distribut\n",
      "0.011% particip\n",
      "0.011% corpor\n",
      "0.005% lack\n",
      "0.005% household\n",
      "0.004% supplier\n",
      "0.004% custom\n",
      "0.004% bear\n",
      "0.003% crisi\n",
      "0.001% wealth\n",
      "0.001% global\n",
      "0.0% workforc\n",
      "0.0% stagnat\n",
      "0.0% resili\n",
      "0.0% headwind\n",
      "0.0% geopolit\n",
      "0.0% bull\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "print(basis_changes[i])\n",
    "for j in ind_relwords_05:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 4\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_relwords_25:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])\n",
    "i = 5\n",
    "print(\"\\n\")\n",
    "print(basis_changes[i])\n",
    "for j in ind_relwords_125:\n",
    "    val = percent_word_counts_with_words[i][j]\n",
    "    print(str(np.around(val[0] * 100, decimals=3)) + \"%\" + \" \" + val[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot how they change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts_with_words = []\n",
    "z = 1\n",
    "for arr in list(increase_df[\"WordCount\"]):\n",
    "    lst = arr[0].toarray()[0]\n",
    "    s = sum(lst)\n",
    "    i = 0\n",
    "    new_arr = []\n",
    "    names = input_vectorizer_counts.get_feature_names()\n",
    "    while i < len(lst):\n",
    "        new_arr.append((lst[i] * 100 / s, names[i]))\n",
    "        i += 1\n",
    "    word_counts_with_words.append(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts_with_words = [sorted(list(x)) for x in word_counts_with_words]\n",
    "dummy = [x.reverse() for x in word_counts_with_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_word_counts_with_words = []\n",
    "i = 0\n",
    "while i < len(word_counts_with_words):\n",
    "    lst = word_counts_with_words[i]\n",
    "    new_lst = []\n",
    "    j = 0\n",
    "    while j < len(lst):\n",
    "        if lst[j][1] in relevant_words:\n",
    "            new_lst.append(lst[j])\n",
    "        j += 1\n",
    "    rel_word_counts_with_words.append(new_lst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findItem(theList, word):\n",
    "    return [theList.index(i) for i in theList if i[1] == word][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_word_g2 = [[findItem(x, word) for x in rel_word_counts_with_words] for word in relevant_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_vals = []\n",
    "for word_indices in rel_word_g2:\n",
    "    i = 0\n",
    "    sub_list = []\n",
    "    while i < len(rel_word_counts_with_words):\n",
    "        index = word_indices[i]\n",
    "        date_lst = rel_word_counts_with_words[i]\n",
    "        sub_list.append(date_lst[index])\n",
    "        i += 1\n",
    "    y_vals.append(sub_list)\n",
    "\n",
    "labels = relevant_words\n",
    "\n",
    "yr = list(increase_df[\"Year\"])\n",
    "mnth = list(increase_df[\"Month\"])\n",
    "day = list(increase_df[\"Day\"])\n",
    "x_vals = []\n",
    "for i in zip(mnth, day, yr):\n",
    "    date = pd.to_datetime(str(i[0]) + \"/\" + str(i[1]) + \"/\" + str(i[2]))\n",
    "    x_vals.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ali-wetrill/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create random data with numpy\n",
    "import numpy as np\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[0]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[0]\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[1]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[1]\n",
    ")\n",
    "\n",
    "trace2 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[2]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[2]\n",
    ")\n",
    "\n",
    "trace3 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[3]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[3]\n",
    ")\n",
    "\n",
    "trace4 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[4]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[4]\n",
    ")\n",
    "\n",
    "trace5 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[5]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[5]\n",
    ")\n",
    "\n",
    "trace6 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[6]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[6]\n",
    ")\n",
    "\n",
    "trace7 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[7]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[7]\n",
    ")\n",
    "\n",
    "trace8 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[8]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[8]\n",
    ")\n",
    "\n",
    "trace9 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[9]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[9]\n",
    ")\n",
    "\n",
    "trace10 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[10]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[10]\n",
    ")\n",
    "\n",
    "trace11 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[11]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[11]\n",
    ")\n",
    "\n",
    "trace12 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[12]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[12]\n",
    ")\n",
    "\n",
    "trace13 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[13]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[13]\n",
    ")\n",
    "\n",
    "trace14 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[14]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[14]\n",
    ")\n",
    "\n",
    "trace15 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[15]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[15]\n",
    ")\n",
    "\n",
    "trace16 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[16]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[16]\n",
    ")\n",
    "\n",
    "trace17 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[17]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[17]\n",
    ")\n",
    "\n",
    "trace18 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[18]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[18]\n",
    ")\n",
    "\n",
    "trace19 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[19]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[19]\n",
    ")\n",
    "\n",
    "trace20 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[20]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[20]\n",
    ")\n",
    "\n",
    "trace21 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[21]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[21]\n",
    ")\n",
    "\n",
    "trace22 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[22]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[22]\n",
    ")\n",
    "\n",
    "trace23 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[23]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[23]\n",
    ")\n",
    "\n",
    "trace24 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[24]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[24]\n",
    ")\n",
    "\n",
    "trace25 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[25]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[25]\n",
    ")\n",
    "\n",
    "trace26 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[26]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[26]\n",
    ")\n",
    "\n",
    "trace27 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[27]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[27]\n",
    ")\n",
    "\n",
    "trace28 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[28]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[28]\n",
    ")\n",
    "\n",
    "trace29 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[29]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[29]\n",
    ")\n",
    "\n",
    "trace30 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[30]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[30]\n",
    ")\n",
    "\n",
    "trace31 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[31]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[31]\n",
    ")\n",
    "\n",
    "trace32 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[32]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[32]\n",
    ")\n",
    "\n",
    "trace33 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[33]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[33]\n",
    ")\n",
    "\n",
    "trace34 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[34]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[34]\n",
    ")\n",
    "\n",
    "trace35 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[35]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[35]\n",
    ")\n",
    "\n",
    "trace36 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[36]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[36]\n",
    ")\n",
    "\n",
    "trace37 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[37]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[37]\n",
    ")\n",
    "\n",
    "trace38 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[38]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[38]\n",
    ")\n",
    "\n",
    "trace39 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[39]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[39]\n",
    ")\n",
    "\n",
    "trace40 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[40]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[40]\n",
    ")\n",
    "\n",
    "trace41 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[41]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[41]\n",
    ")\n",
    "\n",
    "trace42 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[42]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[42]\n",
    ")\n",
    "\n",
    "trace43 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[43]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[43]\n",
    ")\n",
    "\n",
    "trace44 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[44]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[44]\n",
    ")\n",
    "\n",
    "trace45 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[45]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[45]\n",
    ")\n",
    "\n",
    "trace46 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[46]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[46]\n",
    ")\n",
    "\n",
    "trace47 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[47]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[47]\n",
    ")\n",
    "\n",
    "trace48 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[48]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[48]\n",
    ")\n",
    "\n",
    "trace49 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[49]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[49]\n",
    ")\n",
    "\n",
    "trace50 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[50]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[50]\n",
    ")\n",
    "\n",
    "trace51 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[51]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[51]\n",
    ")\n",
    "\n",
    "trace52 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[52]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[52]\n",
    ")\n",
    "\n",
    "trace53 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[53]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[53]\n",
    ")\n",
    "\n",
    "trace54 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[54]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[54]\n",
    ")\n",
    "\n",
    "trace55 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[55]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[55]\n",
    ")\n",
    "\n",
    "trace56 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[56]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[56]\n",
    ")\n",
    "\n",
    "trace57 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[57]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[57]\n",
    ")\n",
    "\n",
    "trace58 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[58]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[58]\n",
    ")\n",
    "\n",
    "trace59 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[59]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[59]\n",
    ")\n",
    "\n",
    "trace60 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[60]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[60]\n",
    ")\n",
    "\n",
    "trace61 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[61]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[61]\n",
    ")\n",
    "\n",
    "trace62 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[62]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[62]\n",
    ")\n",
    "\n",
    "trace63 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[63]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[63]\n",
    ")\n",
    "\n",
    "trace64 = go.Scatter(\n",
    "    x = x_vals,\n",
    "    y = [y[0] for y in y_vals[64]],\n",
    "    mode = 'lines',\n",
    "    name = relevant_words[64]\n",
    ")\n",
    "\n",
    "data = [trace2, trace3, trace7, \n",
    "        trace21, trace22, trace23, \n",
    "       trace30, trace36,\n",
    "       trace41, trace42, trace44,\n",
    "       trace53, trace56, trace58,\n",
    "       trace61]\n",
    "\n",
    "py.iplot(data, filename='line-mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relevant_words.index(\"continu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ind = [2, 3, 7, 10, 14,17,19,21, 22, 23, 24, 30, 36, 37, 38, 41, 42, 43, 44, 46, 53, 54, 56, 58,61]\n",
    "# avg = []\n",
    "# for i in ind:\n",
    "#     avg.append((np.mean([y[0] for y in y_vals[i]]), i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sorted(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Words Overall - No Global Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to change date - change the numbers and replace 'table' with the appropriate lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = relevant_data_df[\"WordCount\"]\n",
    "# relevant_data_df[relevant_data_df[\"Year\"] <= 1990][\"WordCount\"]\n",
    "# relevant_data_df[np.logical_and(relevant_data_df[\"Year\"] <= 2000, relevant_data_df[\"Year\"] > 1990)][\"WordCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_count_all = [x[0].toarray()[0] for x in list(table)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_words = sum([sum(x) for x in word_count_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "new_word_count_all = []\n",
    "while i < m.shape[1]:\n",
    "    num_words = 0\n",
    "    for arr in word_count_all:\n",
    "        num_words += arr[i]\n",
    "    new_word_count_all.append(num_words) \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_freq = np.array(new_word_count_all) * 1000 / total_words\n",
    "words = input_vectorizer_counts.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_and_word_freq = [x for x in zip(word_freq, words)]\n",
    "words_and_word_freq = sorted(words_and_word_freq)\n",
    "words_and_word_freq.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14.449867372847605, 'mr'),\n",
       " (11.59437368450341, 'think'),\n",
       " (11.126823418758095, 'would'),\n",
       " (10.172082502708736, 'chairman'),\n",
       " (10.088610868334134, 'rate'),\n",
       " (6.8774079929818361, 'market'),\n",
       " (6.7819339013768998, 'that'),\n",
       " (6.6520891367941877, 'go'),\n",
       " (6.1774465099582203, 'year'),\n",
       " (5.9821337968464086, 'it'),\n",
       " (5.8648370557317735, 'percent'),\n",
       " (5.4725749307949227, 'on'),\n",
       " (5.3765552729522446, 'price'),\n",
       " (5.3596427195822276, 'inflat'),\n",
       " (5.1926994508330253, 'point'),\n",
       " (4.9968411714834708, 'growth'),\n",
       " (4.4807355105790752, 'don'),\n",
       " (4.2799671350898381, 'us'),\n",
       " (4.2259560775533318, 'we'),\n",
       " (4.0491926165247651, 'polici'),\n",
       " (4.0082751486940777, 'expect'),\n",
       " (3.9122554908513996, 'time'),\n",
       " (3.8527887709374684, 'sai'),\n",
       " (3.7698627028006095, 'forecast'),\n",
       " (3.7387654272492878, 'greenspan'),\n",
       " (3.7354920298228329, 'like'),\n",
       " (3.6831176709995539, 'well'),\n",
       " (3.6312888784140172, 'economi'),\n",
       " (3.5243578958164892, 'get'),\n",
       " (3.4867138254122572, 'see'),\n",
       " (3.4207003106454157, 'mai'),\n",
       " (3.4179724794567035, 'term'),\n",
       " (3.2423001509036213, 'risk'),\n",
       " (3.1599196490045052, 're'),\n",
       " (3.1430070956344882, 'move'),\n",
       " (3.1419159631590032, 'increas'),\n",
       " (3.1217300123625309, 'could'),\n",
       " (3.0780847133431317, 'look'),\n",
       " (3.0578987625466594, 'much'),\n",
       " (2.9176882394468393, 'last'),\n",
       " (2.8407633999251485, 'fund'),\n",
       " (2.8102116906115691, 'chang'),\n",
       " (2.7998459320944615, 'seem'),\n",
       " (2.7251033575237402, 'meet'),\n",
       " (2.6749112636514312, 'question'),\n",
       " (2.6639999388965814, 'also'),\n",
       " (2.5996231228429676, 'you'),\n",
       " (2.5794371720464953, 'continu'),\n",
       " (2.5216071508457913, 'wai'),\n",
       " (2.501421200049319, 'make')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_and_word_freq[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtable = relevant_data_df[\"WeightedWordCount\"]\n",
    "# relevant_data_df[relevant_data_df[\"Year\"] <= 1990][\"WeightedWordCount\"]\n",
    "# relevant_data_df[np.logical_and(relevant_data_df[\"Year\"] <= 2000, relevant_data_df[\"Year\"] > 1990)][\"WeightedWordCount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wword_count_all = [x[0].toarray()[0] for x in list(wtable)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wtotal_words = sum([sum(x) for x in wword_count_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "new_wword_count_all = []\n",
    "while i < m.shape[1]:\n",
    "    num_words = 0\n",
    "    for arr in wword_count_all:\n",
    "        num_words += arr[i]\n",
    "    new_wword_count_all.append(num_words) \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wword_freq = np.array(new_wword_count_all) * 1000 / wtotal_words\n",
    "wwords = input_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wwords_and_word_freq = [x for x in zip(wword_freq, wwords)]\n",
    "wwords_and_word_freq = sorted(wwords_and_word_freq)\n",
    "wwords_and_word_freq.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(12.253269574676425, 'mr'),\n",
       " (9.8194201993837247, 'think'),\n",
       " (9.2829404027906701, 'would'),\n",
       " (8.6167581117407099, 'chairman'),\n",
       " (8.3669111669019713, 'rate'),\n",
       " (5.6804654745908199, 'market'),\n",
       " (5.6504770440546519, 'that'),\n",
       " (5.542395486094108, 'go'),\n",
       " (5.1600821619356747, 'year'),\n",
       " (5.0082402021673644, 'it'),\n",
       " (4.764257136774015, 'percent'),\n",
       " (4.597651345796014, 'on'),\n",
       " (4.5076507046425851, 'greenspan'),\n",
       " (4.3548128695870707, 'price'),\n",
       " (4.3199890259324212, 'point'),\n",
       " (4.2401801549993694, 'inflat'),\n",
       " (4.2022541062999768, 'growth'),\n",
       " (3.7799251696407881, 'don'),\n",
       " (3.5211156578610212, 'we'),\n",
       " (3.5120730147563255, 'volcker'),\n",
       " (3.4713360257119428, 'us'),\n",
       " (3.3525761971130565, 'polici'),\n",
       " (3.2831792833644116, 'time'),\n",
       " (3.2796427947393689, 'expect'),\n",
       " (3.1861662566032045, 'sai'),\n",
       " (3.1854699340908406, 'forecast'),\n",
       " (3.136959824773986, 'economi'),\n",
       " (3.0967254598755023, 'like'),\n",
       " (3.0883304125344493, 'well'),\n",
       " (2.9872928645056023, 'get'),\n",
       " (2.9583164598110296, 'see'),\n",
       " (2.8844888924580063, 'mai'),\n",
       " (2.8311732876871667, 'term'),\n",
       " (2.8158596458955376, 're'),\n",
       " (2.7739631085404994, 'move'),\n",
       " (2.6511126473778028, 'increas'),\n",
       " (2.6162440825338376, 'look'),\n",
       " (2.5651343415829975, 'could'),\n",
       " (2.5573592741721365, 'much'),\n",
       " (2.548373532330531, 'risk'),\n",
       " (2.5181194104512508, 'last'),\n",
       " (2.4163967164278795, 'seem'),\n",
       " (2.3303868960080636, 'chang'),\n",
       " (2.3003689470151953, 'fund'),\n",
       " (2.2990287746304561, 'meet'),\n",
       " (2.2262923722563972, 'also'),\n",
       " (2.2254446441267977, 'continu'),\n",
       " (2.2217286005021779, 'question'),\n",
       " (2.110040732631381, 'you'),\n",
       " (2.0818631311183147, 'come')]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wwords_and_word_freq[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# graphs for reduction, increase, total words. t stats, stds under increase and reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each basis point change top words under both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# box and whisker plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tables for stemmer mappings, word counts during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# most frequent terms across dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doc filtering example appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # for each document in train_df, run CountVectorizer to remove stop words.\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# import chardet\n",
    "# import PyPDF2\n",
    "\n",
    "# def input_documents(filenames):\n",
    "#     text = \"\"\n",
    "#     for filename in filenames:\n",
    "#         with open(filename, 'rb') as input_file:\n",
    "#             pdfReader = PyPDF2.PdfFileReader(input_file)\n",
    "#             num_pages = pdfReader.numPages #later refactor to words within a doc\n",
    "#             for i in range(0, num_pages):\n",
    "#                 pageObj = pdfReader.getPage(i)\n",
    "#                 text += \" \" + pageObj.extractText()\n",
    "#     return [text]\n",
    "\n",
    "# i = 0\n",
    "# new_col_feature_names = []\n",
    "# new_col_tokens = []\n",
    "# print(\"Number Rows\", relevant_data_df.shape[0])\n",
    "# while i < relevant_data_df.shape[0]:\n",
    "#     try:\n",
    "#         document_paths = []\n",
    "#         document_paths += np.array(relevant_data_df[\"Transcripts\"])[i]\n",
    "# #         tfidf_transformer = TfidfTransformer()\n",
    "#         input_vectorizer = CountVectorizer(input=\"content\", stop_words=\"english\")\n",
    "#         docs = input_documents(document_paths)\n",
    "# #         tfidf_docs = tfidf_transformer.fit_transform(docs)\n",
    "# #         print(tfidf_docs)\n",
    "#         x = input_vectorizer.fit_transform(docs)\n",
    "#         new_col_feature_names.append(list(zip(range(0,len(input_vectorizer.get_feature_names())),input_vectorizer.get_feature_names())))\n",
    "#         new_col_tokens.append(x) \n",
    "#         print(\"Row \" + str(i + 1) +  \" Complete\")\n",
    "#         i += 1\n",
    "#     except:\n",
    "#         new_col_feature_names.append([None])\n",
    "#         new_col_tokens.append(None)\n",
    "#         print(\"Row \" + str(i + 1) +  \" Complete\")\n",
    "#         i += 1\n",
    "\n",
    "# relevant_data_df[\"TokenCount\"] = new_col_tokens\n",
    "# relevant_data_df[\"FeatureNames\"] = new_col_feature_names"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
